{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import svm, datasets\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import auc\n",
    "#from sklearn.metrics import plot_roc_curve\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from lifelines.statistics import logrank_test\n",
    "from lifelines.statistics import multivariate_logrank_test\n",
    "from lifelines import*\n",
    "from lifelines import KaplanMeierFitter\n",
    "from lifelines.statistics import logrank_test\n",
    "from lifelines.statistics import multivariate_logrank_test\n",
    "from lifelines.utils import median_survival_times\n",
    "from lifelines import CoxPHFitter  #cox-proportional hazard ratio\n",
    "import matplotlib.pyplot as plt\n",
    "kmf=KaplanMeierFitter()\n",
    "cph=CoxPHFitter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data cleaning time\n",
    "time = pd.read_excel(r\"C:\\Data\\Cox\\Template for Suvival analysis all LNs.xlsx\", header=None)\n",
    "time2 = time.replace({0:{24167:\"01-24167B17\",\"7P\":\"03292019_RZ_DC PD-L1 Panel_14346-1-7 SLN(N)x9\",\n",
    "                                \"hc-04\":\"12195-01-04x1\",\"hc-08\":\"12195-01-08\",\n",
    "                                \"hc-18\":\"12195-01-18x1\",\"hc-22\":\"12195-01-22x1\", \"hc-24\":\"12195-01-24x1\",\n",
    "                                \"hc-27\":\"12195-01-27x1\",\"hc-29\":\"12195-01-29\",\"hc-35\":\"12195-01-35x1\",\"hc-43\":\"12195-01-43x1\",\n",
    "                                \"hc-44\":\"12195-01-45x1\",\"hc-47\":\"12195-01-47x1\",\"hc-50\":\"12195-01-50x1\",\n",
    "                                \"VPt3\":\"12195-03-LNNA1x4\",\"VPt12\":\"12195-12-LNNa1x4\",\"VPt15\":\"12195-15-LNNA1x7\",\n",
    "                                \"VPT17\":\"12195-17-LNNA1x3\",\"VPt19\":\"12195-19-LNNA1x7\",\"VPt20\":\"12195-20-LNNA1x4\",\n",
    "                                \"VPt21\":\"12195-21-LNNA1x7\",\"VPt22\":\"12195-22-LNNA1x4\",\"VPt26\":\"12195-26-LNNA1x8\",\n",
    "                                \"VPt28\":\"12195-28-LNNA1x7\",\"VPt29\":\"12195-29-LNNB1x7\",\"VPt31\":\"12195-31-LNNA1x4\",\n",
    "                                \"Pt10\":\"14346-1-10-LNx5\",\"Pt13\":\"14346-1-13-SNL(N)x6\", \"Pt14\":\"14346-1-14-LN(N)x8\",\n",
    "                                \"Pt16\":\"14346-1-16-LN(N)x6\",\"Pt2\":\"14346-1-2-LN-Nx5\",21:\"14346-1-21 Cx6\",\"Pt22\":\"14346-1-22-B2x1\",\n",
    "                                \"Pt25\":\"14346-1-25-A1x1\",\"Pt26\":\"14346-1-26-E1x4\",\"Pt29\":\"14346-1-29-E21x2\",\n",
    "                                \"Pt30\":\"14346-1-30 B1x5\",\"5P\":\"14346-1-5-LN(N)x10\",\"9P\":\"14346-1-9-SLN(N)x11\",\n",
    "                        \"8403G\":\"8403Gx2\", 23:\"20190222_RZ_DC PD-L1 Panel_14346-1-23 (Block #A2)x4\",\n",
    "                        28:\"20190222_RZ_DC PD-L1 Panel_14346-1-28 (Block #B1)x6\",\"Pt37\":\"20190222_RZ_DC PD-L1 Panel_14346-1-37 C1x3\",\n",
    "                        \"Pt36\":\"14346-1-36-C1x2\",33:\"14346-1-33 A1x9\",38:\"14346-1-38 A1x9\",39:\"14346-1-39 A1x6\",\n",
    "                        \"VPt9\":\"20190222_RZ_DC PD-L1 Panel_12195-09 LNNA1x5\",\"VPt13\":\"20190222_RZ_DC PD-L1 Panel_12195-13 LNNA1x5\",\n",
    "                        \"VPt23\":\"20190222_RZ_DC PD-L1 Panel_12195-23 LNNC2x6\",\"Pt32\":\"14346-1-32 Ax1\"}})\n",
    "time3 = time2.rename(columns={0:\"Pt\",1:\"Days\",2:\"ifRelapse\"}).drop(columns=3)\n",
    "\n",
    "# Data: proportion of DCs found in homo and heterocluster \n",
    "data = pd.read_csv(r\"C:\\Data\\DCs_results\\DCs_dedicated_to_Cluster_TotDCs.txt\")\n",
    "data2 = data[[\"Pt\",\"CD1a+_%h\",\"CD1a+_%het\",\"cDC1_%h\",\"cDC1_%het\"]]\n",
    "# Merge data with Size of clusters\n",
    "df = pd.merge(time3, data2, on=[\"Pt\"], how=\"inner\")\n",
    "data_clustersize = pd.read_csv(r\"C:\\Data\\DCs_results\\SIZE_cType_Pts.txt\")\n",
    "data_clustersize[\"Pt\"] = data_clustersize[\"Unnamed: 0\"]\n",
    "data_clustersize = data_clustersize.drop(columns=\"Unnamed: 0\")\n",
    "df2 = pd.merge(df, data_clustersize, on=[\"Pt\"], how=\"inner\").fillna(0)\n",
    "\n",
    "# DataNew: porportion of PD-L1+DCs (out of all DCs in slide)-cDC1\n",
    "df_Cluster_pv2 = pd.read_csv(r\"C:\\Data\\DCs_results\\c_Type.txt\")\n",
    "df_Cluster_pv2_Pt = df_Cluster_pv2[~df_Cluster_pv2[\"Pt\"].isin([\"Norm LN1x4\",\"Norm LN2x4\",\"Norm LN3x4\",\"Norm LN4x4\"])].drop(columns=[\"Unnamed: 0\"])\n",
    "df_Cluster_pv2_PtHet = df_Cluster_pv2_Pt[(df_Cluster_pv2_Pt.c_Type==\"Het\")][[\"Pt\",\"CD141+PD-L1+\"]].pivot_table(index=[\"Pt\"],aggfunc=lambda x:sum(x))\n",
    "New_merge = pd.merge(df_Cluster_pv2_PtHet, data, on=[\"Pt\"])\n",
    "New_merge[\"%PD-L1HetcDC1 out all cDC1\"] = 100*New_merge[\"CD141+PD-L1+\"]/New_merge[\"cDC1_ct\"].round(1)\n",
    "New_data=New_merge[[\"Pt\",\"%PD-L1HetcDC1 out all cDC1\"]].round(1)\n",
    "df3 = pd.merge(df2, New_data, on=[\"Pt\"], how=\"inner\")\n",
    "\n",
    "# DataNew: porportion of PD-L1+DCs (out of all DCs in slide)-CD1A\n",
    "df_Cluster_pv2_PtHet1A_PDL1 = df_Cluster_pv2_Pt[(df_Cluster_pv2_Pt.c_Type==\"Het\")][[\"Pt\",\"CD1a+PD-L1+\"]].pivot_table(index=[\"Pt\"],aggfunc=lambda x:sum(x))\n",
    "df_Cluster_pv2_Pt[\"Total CD1a+\"] = df_Cluster_pv2_Pt[\"CD1a+PD-L1+\"]+df_Cluster_pv2_Pt[\"CD1a+PD-L1-\"]\n",
    "partial1AdATA = df_Cluster_pv2_Pt[[\"Pt\",\"Total CD1a+\"]].pivot_table(index=[\"Pt\"],aggfunc = lambda x:sum(x))\n",
    "partial1AdATA=partial1AdATA[(partial1AdATA[\"Total CD1a+\"]!=0)]\n",
    "New_data_1A = pd.merge(df_Cluster_pv2_PtHet1A_PDL1,partial1AdATA, on=[\"Pt\"], how=\"inner\")\n",
    "New_data_1A[\"%PD-L1HetCD1A out all CD1A\"] = 100*New_data_1A[\"CD1a+PD-L1+\"]/New_data_1A[\"Total CD1a+\"].round(1)\n",
    "New_data_1A[\"Pt\"]=New_data_1A.index\n",
    "New_data_1A_2=New_data_1A[[\"Pt\",\"%PD-L1HetCD1A out all CD1A\"]].round(1)\n",
    "New_data_1A_2.reset_index(drop=True, inplace=True)\n",
    "df4 = pd.merge(df3, New_data_1A_2, on=[\"Pt\"], how=\"outer\")\n",
    "df4.to_csv(r\"C:\\Data\\Cox\\Data.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Categorical (fixed cutoff)\n",
    "def ClassbyOneThreshold(dTest, cutoff):\n",
    "    cut = dTest.quantile([cutOff])\n",
    "    catgr, gPt =[],[]\n",
    "    for i in np.arange(3, len(dTest.columns)):\n",
    "        g1=dTest[(dTest.iloc[:,i]>cut.iloc[0,i-1])]\n",
    "        g2=dTest[(dTest.iloc[:,i]<=cut.iloc[0,i-1])]\n",
    "        g1[\"Class%s\"%dTest.columns[i]]=1\n",
    "        g2[\"Class%s\"%dTest.columns[i]]=0\n",
    "        g = pd.concat([g1[\"Class%s\"%dTest.columns[i]], g2[\"Class%s\"%dTest.columns[i]]])\n",
    "        gPt.append(pd.concat([g1,g2]))\n",
    "        catgr.append(g)\n",
    "        catgr_all=pd.DataFrame(catgr).transpose()\n",
    "        complete_all=pd.concat(gPt,axis=1)\n",
    "        dfClass = complete_all.loc[:,~complete_all.columns.duplicated()].drop(columns={\"CD1a+_%h\",\"CD1a+_%het\",\"cDC1_%h\",\"cDC1_%het\",\"CD1a-h_Size\",\n",
    "                                                                                             \"cDC1-h_size\",\"Het_size\",\"%PD-L1HetcDC1 out all cDC1\",\"%PD-L1HetCD1A out all CD1A\"})\n",
    "    dfClass.to_csv(r\"C:\\Data\\Cox\\ClassData.txt\")\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    ClassbyOneThreshold(df4, 0.31)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC curve for best cutoff\n",
    "def RocForCutOff(dTest): \n",
    "    Results ={\"Vars\":[],\"Data_cutoff\":[], \"ROC_AUC\":[]}\n",
    "    for i in np.arange(3, len(dTest.columns)-1):  #the last one has NaN\n",
    "        label = dTest.iloc[:,2].values.astype(int)\n",
    "        predictors = dTest.iloc[:,i].values.reshape(-1,1)\n",
    "        model = LogisticRegression()\n",
    "        model.fit(predictors, label)\n",
    "        probs = model.predict_proba(predictors)\n",
    "\n",
    "        fpr, tpr, thresholds = roc_curve(label, probs[:,1]) \n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        #plt.figure()\n",
    "        plt.plot(fpr, tpr, lw=1, label='ROC/AUC (area = %0.2f)' % (roc_auc))\n",
    "        plt.plot([0, 1], [0, 1], '--', color=(0.6, 0.6, 0.6), label='Luck')\n",
    "        plt.xlim([-0.05, 1.05])\n",
    "        plt.ylim([-0.05, 1.05])\n",
    "        plt.grid()\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title('ROC %s'%dTest.columns[i])\n",
    "        plt.legend(loc=\"lower right\")\n",
    "        #plt.savefig('C:/Users/DrHeh/Google Drive/programming/samples/Cox/ROCAUC%s.tiff'% columnOfInterest, format='tiff', dpi=600)\n",
    "        plt.show() # comment out for multiple lines plot together\n",
    "\n",
    "\n",
    "        optimal_thresholds = thresholds[np.argmax(tpr-fpr)]  \n",
    "        data = pd.DataFrame({\"%s\" %dTest.columns[i]:dTest.iloc[:,i].values, \n",
    "                            \"Labels\":label, \"Probs\":probs[:,1]}).sort_values(by=[\"Probs\"])\n",
    "        cutoff = data.loc[data[data.columns[2]] == optimal_thresholds][[data.columns[0]]].iloc[0,0]\n",
    "\n",
    "        Results[\"Vars\"].append(dTest.columns[i])\n",
    "        Results[\"Data_cutoff\"].append(cutoff)\n",
    "        Results[\"ROC_AUC\"].append(roc_auc)\n",
    "\n",
    "\n",
    "    Res = pd.DataFrame(Results)\n",
    "    Res.to_csv(r\"C:\\Data\\Cox\\ROC_CutOffAUC.txt\")\n",
    "if __name__ == '__main__':\n",
    "    RocForCutOff(df4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create feature-dependent cutoffs based on ROC curve:\n",
    "Res=pd.read_csv(r\"C:\\Data\\Cox\\ROC_CutOffAUC.txt\")\n",
    "Res=Res.drop(columns=\"Unnamed: 0\")\n",
    "def ClassbyRoc(dTest):\n",
    "    catgr, gPt =[],[]\n",
    "    for i in np.arange(3, len(dTest.columns)-1):\n",
    "        g1=dTest[(dTest.iloc[:,i]>Res.iloc[:,1][i-3])]\n",
    "        g2=dTest[(dTest.iloc[:,i]<=Res.iloc[:,1][i-3])]\n",
    "        g1[\"Class%s\"%dTest.columns[i]]=1\n",
    "        g2[\"Class%s\"%dTest.columns[i]]=0\n",
    "        g = pd.concat([g1[\"Class%s\"%dTest.columns[i]], g2[\"Class%s\"%dTest.columns[i]]])\n",
    "        gPt.append(pd.concat([g1,g2]))\n",
    "        catgr.append(g)\n",
    "        catgr_all=pd.DataFrame(catgr).transpose()\n",
    "        complete_all=pd.concat(gPt,axis=1)\n",
    "        dfClassbyROC = complete_all.loc[:,~complete_all.columns.duplicated()].drop(columns={\"CD1a+_%h\",\"CD1a+_%het\",\"cDC1_%h\",\"cDC1_%het\",\"CD1a-h_Size\",\n",
    "                                                                                             \"cDC1-h_size\",\"Het_size\",\"%PD-L1HetcDC1 out all cDC1\",\"%PD-L1HetCD1A out all CD1A\"})\n",
    "    dfClassbyROC.to_csv(r\"C:\\Data\\Cox\\ClassbyRocData.txt\")\n",
    "\n",
    "\n",
    "    FrequencyCount=[]\n",
    "    for i in np.arange(3, len(dTest.columns)-1):\n",
    "        FrequencyCount.append(dfClassbyROC[\"Class%s\"%dTest.columns[i]].value_counts())\n",
    "        Freq=pd.DataFrame(FrequencyCount)\n",
    "    Freq[\"From Lower cutoff\"]=100*Freq[0]/(Freq[0]+Freq[1])\n",
    "    Freq.to_csv(r\"C:\\Data\\Cox\\RocThresholdCutoffs.txt\")\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    ClassbyRoc(df4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfClassbyROC=pd.read_csv(r\"C:\\Data\\Cox\\ClassbyRocData.txt\")\n",
    "dfClassbyROC=dfClassbyROC.drop(columns=\"Unnamed: 0\")\n",
    "# ROC for all predictors (can also put multiple, most powerful ones)\n",
    "def RocAllPredictor(dTest):\n",
    "    predictors = dTest.iloc[:,np.r_[3:len(dTest.columns)-1]].values\n",
    "    label = dTest.iloc[:,2].values.astype(int)\n",
    "\n",
    "    model = LogisticRegression()\n",
    "    model.fit(predictors, label)\n",
    "    probs = model.predict_proba(predictors)\n",
    "\n",
    "    fpr, tpr, thresholds = roc_curve(label, probs[:,1]) \n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.plot(fpr, tpr, lw=1, label='ROC/AUC (area = %0.2f)' % (roc_auc))\n",
    "    plt.plot([0, 1], [0, 1], '--', color=(0.6, 0.6, 0.6), label='Luck')\n",
    "    plt.xlim([-0.05, 1.05])\n",
    "    plt.ylim([-0.05, 1.05])\n",
    "    plt.grid()\n",
    "\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC_all predictors')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.savefig(r\"C:\\Data\\Cox\\allPredictorsROCAUC.pdf\", format=\"pdf\")\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "\n",
    "# Cox Hazard Ratio- Mutivariate\n",
    "def Cox(df):\n",
    "    df = df.drop(columns=\"Pt\")\n",
    "    c=cph.fit(df, duration_col=\"Days\", event_col=\"ifRelapse\")\n",
    "    bx=c.plot(hazard_ratios=True)\n",
    "    \n",
    "\n",
    "# Cox Hazard Ratio- CI of every feature \n",
    "def CoxIndvidualFeature(dTest):\n",
    "    for i in np.arange(3, len(dTest.columns)-1): #the last one columns has NaN\n",
    "        columnOfInterest = dTest.columns[i]\n",
    "        dataOfInterest = dTest[[\"Days\",\"ifRelapse\",columnOfInterest]]\n",
    "        c=cph.fit(dataOfInterest, duration_col=\"Days\", event_col=\"ifRelapse\")\n",
    "        c.print_summary()\n",
    "        plt.figure()\n",
    "        bx=c.plot(hazard_ratios=True)\n",
    "        bx.get_figure().tight_layout()\n",
    "        bx.get_figure().savefig('C:/Data/Cox/%s.tiff'% columnOfInterest, format='tiff', dpi=600)\n",
    "        \n",
    "\n",
    "\n",
    "# KM plot for every feature\n",
    "def KMplot(dTest):\n",
    "    for i in np.arange(3, len(dTest.columns)):\n",
    "        g_high = dTest[(dTest.iloc[:,i]==1)]\n",
    "        g_low = dTest[(dTest.iloc[:,i]==0)]\n",
    "\n",
    "        plt.figure()\n",
    "        print(\"KM curves\")\n",
    "        kmf1=kmf.fit(g_high[\"Days\"], g_high[\"ifRelapse\"], label=\"Hi %s\"%dTest.columns[i])\n",
    "        ax=kmf.plot(color=\"red\", ci_show=False,show_censors=True)\n",
    "\n",
    "        kmf1=kmf.fit(g_low[\"Days\"], g_low[\"ifRelapse\"], label=\"Low\")\n",
    "        ax=kmf.plot(color=\"black\", ci_show=False,show_censors=True)\n",
    "        plt.ylim(0,1.1)\n",
    "\n",
    "        ax.get_figure().savefig(\"C:/Data/Cox/KM%s.png\"%dTest.columns[i])\n",
    "        \n",
    "\n",
    "# Logrank for every features (once calssified):\n",
    "def logrank(dTest):\n",
    "    Logp, Vars  =[],[]\n",
    "    for i in np.arange(3, len(dTest.columns)):\n",
    "        g_high = dTest[(dTest.iloc[:,i]==1)]\n",
    "        g_low = dTest[(dTest.iloc[:,i]==0)]\n",
    "        lr = logrank_test(g_high.Days, g_low.Days, g_high.ifRelapse, g_low.ifRelapse)\n",
    "        Vars.append(dTest.columns[i])\n",
    "        Logp.append(lr.p_value)\n",
    "    Log=pd.DataFrame({\"logp\":Logp, \"Vars\":Vars})\n",
    "    Log.to_csv(\"C:/Data/Cox/Logrankp.txt\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    RocAllPredictor(dfClassbyROC)\n",
    "    Cox(dfClassbyROC)\n",
    "    CoxIndvidualFeature(dfClassbyROC)\n",
    "    KMplot(dfClassbyROC)\n",
    "    logrank(dfClassbyROC)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
